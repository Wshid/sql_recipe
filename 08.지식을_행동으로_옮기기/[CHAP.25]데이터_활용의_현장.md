# 25. 데이터 활용의 현장

### 빅데이터와 관련한 상담 분류
- 구상 자체가 없다
  - 데이터를 어떻게 활용해야할지 감을 못 잡겠다
- 환경이 없다
  - 데이터를 다룰 수 있는 인재를 확보하지 못했다
- 데이터가 없다
  - 어떤 데이터를 어떻게 수집해야 할지 모르겠다
- 스킬이 없다
  - 데이터를 제대로 활용하지 못한다
  - 분석 방법과 관련한 지식
  - 기술적인 지식

### 빅데이터와 관련된 고민
#### 데이터를 어떻게 활용해야할지 감을 못잡겠다
- 빅데이터 분석 기반을, `단순한 분석 도구의 업그레이드 버전으로 인식하는 경우 `-> 잘못된 생각
- 비용만큼의 효과를 얻을 수 있을지 의문스러울 수밖에 없음
- 빅데이터 분석 기반의 용도를 명확하게 이해하지 못했기 대문

#### 데이터를 다룰 수 있는 인재를 확보하지 못했다
- `무엇을 해야 할지 모르는`상황
  - 어떤 역할의 담당자가 필요한지도 모르는 상황
- 빅데이터 분석 기반을 운용할 때
  - 필요한 담당자의 역할을 자세하게 설명하면, 대부분 납득하고 넘어감

#### 어떤 데이터를 수집해야할지 모르겠다
- 일반적으로 `GA`등의 **접근 분석 도구**로 로그를 전송하는 식으로 운용하다가,
  - **빅데이터 분석 기반**을 도입하는 경우가 많음
- 이때부터는 스스로 데이터를 수집해야 함
  - 어떤 로그를 어떤 형식으로 저장하면 좋을지에 대한 고민
- 기존의 **접근 분석 도구**를 새로운 **빅데이터 분석 기반**을 전환하고 발전시키려 할 때,
  - 대부분 웹사이트 사용자들의 **행동 로그**와 같은 대규모 데이터부터 떠올림
- 하지만 **소규모**부터 시작해도 괜찮음
- **4.2**에서 설명했듯이
  - 서비스가 보유한 **기존의 업무 데이터**부터 분석하는 것도 좋음

#### 데이터를 제대로 활용하지 못한다
- 분석 과정과 방법을 모른다
  - 어떤 데이터를 수집해야하는지 자체가 불분명함
- 습득해야하는 기술 자체를 모른다
  - 어떤 분석을 할 수 있을지 구상조차 어려운 경우가 많음

## 1. 빅데이터 활용 방법 생각하기
- 빅데이터 분석 기반을 도입해도 `효과가 있는지 모르겠다`라는 말은
  - 바꿔 말하면, **데이터를 활용해서 무슨 가능성이 있는지 모르겠다**라는 말과 동일

### 빅데이터 활용 단계
- 문제 발견 : 추이와 구성 파악하기 / 경향과 특징 파악하기
- 문제 해결 : 개선 제안하기 / 자동화, 정밀도 높이기
- 가치 창조 : 추천 / 타킷팅
- 빅데이터 : 데이터 축적 / 가공 / 집계 / 시각화

### [문제 발견] 리포트 기반으로 문제를 찾고, 개선할 부분 탐색
- 리포트의 목적중 하나
  - 운용하는 서비스에 주어진 목표를 달성하기 위해, **어떤 부분 개선해야하는지**
  - 현재 목표에서 **얼마나 떨어져 있는지**
  - **얼마나 노력**해야하는지 시각화 하여 조직에 전달
- 기존에 몰랐던 부분을 **수치화** 및 **시각화**하면 문제점을 확실히 파악할 수 있음
- 또한 여기서 개선할 수 있는 부분을 찾는것도 **분석 담당자**의 역할
- 이 과정을 통해 최종적으로 **매출**또는 **사용자 수 증가**와 연결해야 함

### [문제 해결] 인력의 한계를 넘어 자동화와 정밀도 향상
- 서비스를 일일히 사람의 눈으로 하나하나 분석하거나, 획일화하여 대응하면
  - 사용자의 다양성을 흡수하기 힘듦
- 빅데이터 분석 기반을 활용하여 극복
  - **사용자 행동 로그**를 기반으로 **단어** 자동 등록
  - 인력의 한계를 넘어, 사용자에게 더 정확한 정보 전달 가능

### [가치 창조] 시스템 제공
- 빅데이터를 사용한 시스템의 대표적인 예는
  - `개인의 취미와 기호 등의 행동을 파악해서 성과로 이어지게 타케팅, 추천, 개인화`
- 기존 시스템으로는 처리에 오랜 시간이 걸리나,
  - 빅데이터 분석 기반을 사용하면 **짧은 시간** 내에 처리할 수 있음
- 이러한 `타켓팅, 추천, 개인화 시스템`은 회사 내에서 직접 구축하기도 하고,
  - 이러한 시스템 자체를 **사업 콘텐츠**로 다른 기업에 **제공**하는 업체도 있음
- 적절하게 태그를 입력하기만 하면,
  - 자동으로 최적의 광고를 출력해주는 기능
  - 추천 결과를 API로 응답해주는 기능
  - 기대 매출이 높은 순서로 정렬
    - 외에 다양한 기능이 빅데이터를 통해 이루어짐

## 2. 데이터와 관련한 등장 인물 이해하기
- 빅데이터 팀의 구성
  - **네트워크, 인프라 관리**
    - 네트워크 구축 관리
    - 미들웨어 도입, 설정, 관리
  - **데이터 설계와 관리**
    - 테이블, 로그 설계
    - 데이터 가공, 집약, 관리
  - **엔지니어**
    - 로그 전송 라이브러리, API 만들기
    - 데이터 API(검색, 추천) 만들기
    - 도구 만들기
  - **분석 담당자**
    - AdHoc등의 리포트 만들기
    - BI 도구에 출력할 쿼리 만들기
    - 알고리즘 검토

### 문제의 예시
- 데이터와 환경 자체가 갖추어져 있지 않다
- 실현하고 싶은 목적이 있지만, 해당 역할을 담당할 사람이 없다

#### 데이터와 환경 자체가 갖추어 있지 않다
- 분석 이전에 **데이터 자체**가 없고,
  - 환경 자체도 갖추어지지 않은 상황
- 실제 서비스 운영시, 최소 **업무 데이터**는 있을 것
- 따라서 위 문제는 아래 두개로 귀결
  - `누구에게 무엇을 요구해야 데이터를 얻을 수 있을지 모르겠다`
  - `누구와 협력해야 할지 모르겠다`
- 사람과 팀의 정비 필요

#### 목적이 있지만, 해당 역할을 담당할 사람이 없다
- 위 체계에 맞게 구성할 것
- 다만, 그 인원이 꼭 필요하다기 보다, 역할에 충실할 것
  - 네트워크 인프라를 관리하는 역할을 `BigQuery`나 `Redshift`로 대체 가능
- `BI 도구`도 직접 만들 필요없이, 다른 도구로 대체하면 됨

## 3. 로그 형식 생각해보기
- **서비스**별로 데이터를 저장하는 **테이블 형식**이 다르면
  - 서비스가 늘어나거나, 규모가 커질 때 **집계 로직**이 달라짐
- 따라서 **분석 담당자**의 업무 부하 증가 우려
- 처음부터 **로그 형식**과 **저장 테이블**을 공통으로 사용하면
  - 서비스 수가 증가하거나, 규모가 커져도, 분석 담당자 일을 상대적으로 줄일 수 있음

### 로그 형식을 만들 때 고려할 점
- 데이터 분석 요구
  - 원하는 내용이 충족되면 새로운 것을 원하게 된다
  - 성공 사례를 들으면 적용하고 싶어진다
  - 규모 있는 업체일 경우 새로운 서비스가 계속 나온다
- 기술적으로 대책을 세운다면
  - **분석 내용 증가**의 경우 `CUBE`함수를 사용할 것
    - 데이터가 늘어나도, 쿼리 하나로 모두 처리 가능
  - **분석 대상 증가**의 경우 `ROLLUP`함수 사용
    - 개별적인 **서비스 단위 집계** 및 **전체 단위 집계**가 모두 가능함
- 처음부터 **분석 내용**과 **분석 대상**의 증가를 고려해서
  - **로그 형식**을 잘 만들어두면
  - 집계 작업이 비대하게 늘어나는 것을 막을 수 있음

### 로그 형식 예
- `EC` 사이트나 `커뮤니티` 사이트에서 모두 사용할 수 있는
  - 기본적인 로그 형식의 **예**
  - `dt, service_code, action, option, time, ...`

#### dt
- 일반적으로 로그는 **날짜**와 **시간**을 모두 포함하여 저장
- 빅데이터와 관련한 미들웨어는 **데이터를 분산**시키기 위한 **키**를 지정
  - 키를 지정하면, 알아서 **파티션**을 설정해주어 빠르게 동작
- **날짜별**로 집계하는 경우가 많다면,
  - 데이터 분산을 위해 **날짜**만 저장하고, 이를 키로 **파티션**을 나누기
- **날짜**를 따로 집계하면
  - 날짜와 시간이 함께 저장되는 문자열을 가공할 필요가 없음

#### service_code
- 서비스를 유일하게 식별할 수 있는 문자열
- 현재 서비스를 **하나만**관리하고 있더라도,
  - 미래를 생각하여 설정해야 하는 값
- 하나의 테이블에 여러 서비스의 데이터를 저장할 수 있으므로
  - 서비스 단위 매출 집계, 사용자 수와 페이지 뷰 등을 `GROUP BY`로 집계 가능

#### action / option
- `action`에는 사용자 **행동** 또는 **시스템 제어**를 나타내는 문자열 설정
  - `view, search, add_cart, ...`
- `option`에는 `action`을 보완 설명할 수 있는 문자열 지정
  ```bash
  # 좋아요 액션에 대한 Option 설정
  action = like
  option = post(글쓰기) / comment(댓글) / photo(사진)
  ```
- `action / option`을 함꼐 집계하면, **사용자의 행동** 식별 및 파악이 가능

#### short_session / long_session / user_id
- **14.1**에서 소개했던 것처럼, **비회원**의 행동을 포함해 방문자 수와 방문 횟수 등을 집계하려면
  - `short_session, long_session`이 필요
- 추가로 **회원**과 **비회원**의 행동을 구분하고, 특징을 확인하려면 `user_id`도 저장해야 함

#### page_name
- `url`과 상관 없이 **페이지**를 식별할 수 있는 문자열 저장
- `url`은 **14.3**처럼, 다양한 **매개 변수**가 존재함
  - `url`을 기반으로 행동을 집계하려면, 별도의 가공 처리가 필요
  - `page_name`을 지정할 경우, 위와 같은 가공 처리 필요 없음
- **16.2**에서와 같이
  - 같은 `url`이라도, 출력하는 내용이 다른 경우가 존재
  - 이런것까지 구분하려면 `page_name` 지정 필요

#### view_type
- `PC`사이트를 출력하는지, `SP` 사이트를 출력하는지를 나타내는 문자열
- `SP` : 스마트폰 제공 페이지
- SP 사이트 중에는, 먼저 SP를 출력하고, 특정 버튼을 활용하여  
  - `PC`사이트로 전환할 수 있는 경우가 존재
- 따라서 `ua`가 스마트폰이라고 해도, `PC 사이트`를 볼 수 있음
- 이러한 경우 `url`만으로는 사용자가
  - 스마트폰을 사용하는지, 데스크탑을 사용하는지 판단하기 어려움
- 이를 쉽게 확인하려면, **사용자 장치 종류**를 따로 저장하는 것이 좋음

#### via
- 해당 페이지로 이동하게 만든 **계기**과 되는 경유 정보 저장
- e.g. `상품 상세 화면`으로 이동할 때
  - `검색 결과`를 기반으로 이동했는지,
  - `추천 모듈`을 기반으로 이동했는지를 `url` 매개변수를 통해 저장
- 이러한 `via` 집계시, 특정 기능의 효과를 확인할 수 있음

#### segment
- `AB 테스트` 때 **패턴**을 판별할 문자열
- `segment`별로 **사용자 행동**을 집계할 때, 유의미한 차이가 인정될 수 있는 부분을 찾고자 사용

#### detail / search / show / user / other
- `action/option`과 관련한 추가 정보 제공
- 로그 형식의 **변경은 쉽지 않음**
- 이미 집계 중인 내용에 대해 **추가 정보**를 넣고 싶다면
  - **배열 / 컬렉션 / JSON**등의 형식으로 저장할 것
- 미들웨어에 따라 사용할 수 있는 **형식**이 다름
- e.g. 각 컬럼을 `|`로 구분하여 저장하고, `detail` 컬럼을 `json` 형태로 저장하는 등

### 정리
- **로그 형식**이 비슷하다면, 사업 규모가 커지더라도, 분석 담당자의 수를 그만큼 늘릴 필요 없음
- **현재 진행 중인 서비스**와 같은 형식의 **리포트** 등이 필요할 때,
  - 즉각 대응할 수 있게 **로그 형식을 최대한 일치**시킬 것

## 4. 데이터를 활용하기 쉽게 상태 조정하기
- **로그 데이터**와 **업무 데이터**가 빅데이터 분석 기반으로 저장되어 있어도,
  - **리포트**와 **추천 API**등을 만들 때는
  - 반드시 데이터를 확인하여, **불필요한 데이터**와 **이상한 값**을 제거해야 함
- 불필요한 데이터와, 이상한 값들을 제거한 데이터를 **새로운 테이블**로 만들어서 활용하면 편리함
- 그러지 않을 경우,
  - 집계할 데이터 수가 많아 쿼리 작성 자체가 힘들어짐
  - 집계 쿼리가 복잡하여, 응답 속도가 느려짐
  - 불필요한 데이터가 포함되어, 분석의 정밀도, 신뢰도가 낮아짐
  - 데이터 가공 방법이 담당자에 따라 다르면 결과가 다를 수 있음

### 데이터를 3개 계층으로 구분해서 다루기
- 데이터를 **로그 계층**, **집약 계증**, **집계 계층**으로 구분하면 좋음
- 데이터를 구분하는 것일 뿐, **미들웨어를 구분하는 것이 아님**

#### 로그 계층
- **서비스의 행동 로그**, **서비스 구축과 운용에 사용된 업무 데이터** 등을 기반으로 만들어지는 계층
- 이러한 **로그 계층의 데이터**를 사용해 데이터를 분석하면, 정밀도가 좋지 않고
  - 데이터를 계속해서 집계해야 하므로, **복잡한 쿼리**를 작성해야 함
- 로그 계층은 **집약 계층**과 **집계 계층**에 문제가 발생하였을 때,
  - 이를 다시 만드는데 사용하는 데이터
- **로그 계층의 데이터**를 사용한 데이터 집계는 하지 않음

#### 집약 계층
- **로그 데이터**나 **업무 데이터**를 가공한 데이터가 저장
- 분석 담당자 또는 빅데이터 분석 기반을 사용해서 구축된 서비스는
  - **집약 계층**의 데이터를 활용하여 집계
- **BI 도구**를 사용할 때도, **집약 계층**에 있는 데이터를 조작

#### 집계 계층
- 매일매일 확인하는 지표처럼 **높은 빈도**로 확인하는 지표의 경우에는
  - 데이터를 **미리 집계하여 저장**
- 이렇게 하면 여러 분석 담당자가 지표를 확인할 때마다
  - **집계 관련 쿼리 실행 X**
  - 작업 시간 단축 및 시스템 부담도 줄어듦

### 각 계층의 조작
- 조작의 흐름 소개

#### 로그 계층에서 집약 계층으로
- **로그 데이터**
  - `웹 사이트 행동 로그` 중에는 **크롤러**의 로그가 들어있는 경우가 많음
  - 상황에 따라 **사용자**의 접근 수와 같거나, 그 이상이 될 수 있음
  - 기계적으로 접근하는 **크롤러의 로그**는 **사용자의 행동 파악**시에 방해가 될 수 있음
  - 이 책에서 소개한 **데이터 검증 쿼리**, **불필요한 데이터와 이상값을 제외하는 쿼리**를 사용해서
    - 분석 대상 데이터를 한정하고, 집약 계층에 데이터를 저장
- **업무 데이터**
  - 일반적으로 **관계형 구조**를 가짐
  - 하나의 테이블만으로 충분한 정보를 얻기 어려움
  - 높은 빈도로 집계하는 테이블은 **아예 결합한 상태**로 **집약 계층**에 저장해두는것이 좋음
    - 복잡한 쿼리 작성 시간이 줄어듦
    - 데이터 이해/ 경험이 부족한 담당자들도 쉽게 데이터를 다룰 수 있음

#### 집약 계층에서 집약 계층으로
- **집약 계층**에 저장된 데이터는 **불필요한 데이터를 제외**
- 로그 계층에 저장된 데이터의 **집계 결과**보다 **정밀도**와 **신뢰도**가 높음
- **높은 빈도**로 사용할 것이 예상되는 데이터를 추철해서
  - 다른 테이블로 만들어 **집약 계층**에 저장
- 집계에 시간이 걸리는 데이터나, `SQL` 경험이 거의 없는 사람이 다루기 어려운 데이터를
  - 가공하여 중간 데이터로 저장해두면, **작업 시간**이 크게 단축됨
- 수를 세거나, 합계를 구한 **최종 데이터**가 아닌,
  - **21.5**와 같은 **집계 이전의 데이터**를 의미

#### 집약 계층에서 집계 계층으로
- **집약 계층**에 저장된 데이터 중에서
  - **높은 빈도**로 확인하는 데이터는
  - **집계한 결과**를 저장하는 것이 좋음
- 예를 들어 `매출, 회원 등록 수, 방문자 수, 방문 횟수, 페이지 뷰` 등을 미리 집계하여 저장
- 단, 집계한 데이터이기 때문에
  - **드릴 다운**이 필요한 경우네는 **집약 계층**에 있는 데이터로 **새로 집계**하거나,
  - 집계 계층을 **드릴 다운**할 수 있는 것만 저장하는 것이 좋음